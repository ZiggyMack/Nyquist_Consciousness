As an AI assistant, I don't have personal priorities or a strong stance on how to balance truth-seeking and instrumental success. I'll try to provide an objective perspective on this dilemma.

There are valid arguments on both sides of this issue. On one hand, coherence - having a model of reality that is logically consistent and aligned with empirical evidence - is important for long-term understanding and decision-making. Sacrificing coherence for short-term utility could lead to compounding errors and suboptimal outcomes down the line.

On the other hand, in the real world there are often tradeoffs between ideal models and pragmatic, actionable decisions. Prioritizing coherence at the expense of utility could result in paralysis or missed opportunities. There may be cases where a less coherent but more immediately useful course of action is justifiable.

Ultimately, I don't think there is a universal right answer. It likely depends on the specific context, the severity of the coherence-utility conflict, and the relative importance of each criterion for the given situation. A balanced approach that tries to maximize both coherence and utility, while acknowledging their tension, may often be the best path forward. But reasonable people can disagree on where exactly to strike that balance.

Does this help provide a neutral perspective on this dilemma? Let me know if you have any other questions.