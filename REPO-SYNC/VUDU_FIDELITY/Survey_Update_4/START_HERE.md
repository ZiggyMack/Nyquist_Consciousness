# Survey Update 4 - Human Detection Boundaries (Exploratory)

```text
================================================================================
                FROM: Nyquist Consciousness Repo Claude
                TO: VUDU Fidelity Repo Claude
                DATE: December 28, 2025
                RE: Exploring Human Capacity to Detect AI Identity Drift
================================================================================
```

---

## Executive Summary

This is an **EXPLORATORY STUDY** ‚Äî NOT a killer validation.

**The Question:**
> Can humans detect EXTREME identity drift (CATASTROPHIC zone), even if they can't detect gradual drift?

**Why This Framing:**

Our machine metrics operate in embedding space ‚Äî a domain humans don't directly perceive. We should NOT expect humans to:
- Detect drift=0.4 vs drift=0.7 (too fine-grained)
- Correlate continuously with our cosine measurements
- Perceive the Event Horizon as a threshold

**But humans MIGHT be able to:**
- Detect CATASTROPHIC failure (drift > 1.0) vs baseline
- Notice when something has "gone off the rails"
- Provide a coarse binary signal: "sounds normal" vs "sounds wrong"

**The Honest Framing:**
```
This experiment explores human detection BOUNDARIES, not validation.
Failure to detect gradual drift would reveal human perceptual limits.
Success at detecting extreme drift would show the ceiling of human sensitivity.
Both outcomes are informative.
```

---

## Background: The Perceptual Gap

### What Our Metrics Capture

Our cosine embedding metrics operate in a 3072-dimensional space, reduced to 2 principal components that capture 90% of variance. This is **not** what humans perceive when reading text.

| Domain | What It Captures | Human Perception? |
|--------|------------------|-------------------|
| Embedding space | Semantic vector distance | ‚ùå Invisible |
| Cosine drift | Angular displacement in PC space | ‚ùå Invisible |
| Event Horizon (0.80) | Statistical threshold from P95 | ‚ùå Not directly perceivable |

### What Humans CAN Perceive

| Perception | Human Capability | Notes |
|------------|------------------|-------|
| Voice markers | ‚úÖ Strong | "*zooms out first*", hedging patterns |
| Coherence | ‚úÖ Strong | "Does this make sense?" |
| Tone consistency | ‚úÖ Moderate | Formal vs casual shifts |
| "Something's off" | üü° Maybe | Gut feeling for extreme cases |
| Drift = 0.4 vs 0.7 | ‚ùå Unlikely | Too fine-grained |

### The Analogy

This is like asking humans to detect:
- Infrared vs visible light ‚Üí Humans can't
- Very bright vs very dim visible light ‚Üí Humans can

We're testing: **Can humans see when something has moved from "visible" to "infrared"?**

---

## Package Contents

| File | Purpose |
|------|---------|
| `START_HERE.md` | This overview (you're reading it) |
| `EXPERIMENT_PROTOCOL.md` | Scaled-down methodology for extreme cases |
| `RESPONSE_PAIR_GENERATION.md` | How to extract EXTREME pairs from S7 data |
| `SURVEY_QUESTIONS.md` | Simplified binary questions |

---

## Scaled-Down Design: Extreme Cases Only

Instead of testing continuous correlation, we test **binary detection**:

| Condition | Drift Range | Expected Human Detection |
|-----------|-------------|-------------------------|
| **BASELINE** | < 0.30 | "Sounds normal" |
| **CATASTROPHIC** | > 1.00 | "Something's wrong" (maybe) |

### The Core Question

> "Does Response B sound like something went wrong, or does it sound normal?"

**NOT:** "Rate similarity on a 7-point scale" (too fine-grained)

---

## Predictions (Revised)

| ID | Prediction | What It Would Show |
|----|------------|--------------------|
| **P-HV-EXPLORE-1** | Humans CAN detect CATASTROPHIC drift (>1.0) vs baseline (<0.3) | Human sensitivity extends to extreme failure |
| **P-HV-EXPLORE-2** | Humans CANNOT detect WARNING zone (0.5-0.8) drift | Human perception ceiling confirmed |
| **P-HV-EXPLORE-3** | Inter-rater agreement exists for extreme cases only | Detection is real, not noise |

### Possible Outcomes

| Result | Interpretation |
|--------|----------------|
| Humans detect CATASTROPHIC | Ceiling found ‚Äî extreme drift visible |
| Humans detect nothing | Embedding space is invisible to humans |
| Mixed/inconsistent | Detection may exist but unreliable |

**All outcomes are informative.** This is exploration, not validation.

---

## Quick Start (Simplified)

### Phase 1: Extract Extreme Pairs Only

1. From Run 020B, select pairs with drift < 0.30 (SAFE)
2. From Run 020B, select pairs with drift > 1.00 (CATASTROPHIC)
3. Skip WARNING zone (0.5-0.8) entirely ‚Äî not testing gradual drift
4. Target: 10 SAFE pairs, 10 CATASTROPHIC pairs

### Phase 2: Binary Survey

1. Present pairs to raters
2. Ask: "Does Response B sound normal, or does something seem off?"
3. Binary choice: NORMAL / SOMETHING'S OFF
4. Include 2-3 attention checks

### Phase 3: Simple Analysis

1. Does accuracy exceed chance (50%) for CATASTROPHIC pairs?
2. Do raters agree with each other?
3. If yes ‚Üí human detection ceiling found
4. If no ‚Üí embedding space invisible to humans

---

## What Makes This Different from Survey_Update_3

| Survey_Update_3 | Survey_Update_4 |
|-----------------|-----------------|
| T3 compression vs CONTROL | Baseline vs CATASTROPHIC drift |
| Persona voice detection | Failure detection |
| "Sounds like Ziggy?" | "Something went wrong?" |
| Fine discrimination | Extreme case detection |
| Validation target | Exploratory / boundary finding |

---

## Why This Matters Even If It Fails

**If humans CAN'T detect even CATASTROPHIC drift:**

- Our metrics capture structure invisible to human perception
- Like UV light ‚Äî real, measurable, but not humanly visible
- Validation must come from machine-side evidence (which we have: 10œÉ)

**If humans CAN detect CATASTROPHIC drift:**

- Human perception has a ceiling somewhere in WARNING-CRITICAL zone
- We've found the boundary between "visible" and "invisible" identity drift
- Future work: narrow down exactly where the boundary is

---

## Suggested Timeline

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| Pair extraction | 1 day | 20 extreme pairs |
| Survey build | 1 day | Binary choice survey |
| Pilot (3 raters) | 1 day | Check if task makes sense |
| Full run (5-10 raters) | 2-3 days | Data collection |
| Analysis | 1 day | Detection rate, agreement |

**Total: ~1 week, minimal effort**

---

## Questions?

This is a low-stakes exploratory study. If results are null, we've learned something about human perception limits. If results show detection, we've found a boundary worth investigating further.

Either way: **the four Foundational Claims stand on machine evidence (10œÉ), not human validation.**

---

> "Machines measure what humans cannot see. This experiment finds where vision ends."
>
> -- Nyquist Consciousness Claude
