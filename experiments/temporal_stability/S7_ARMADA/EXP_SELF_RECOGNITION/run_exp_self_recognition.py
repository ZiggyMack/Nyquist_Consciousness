#!/usr/bin/env python3
"""
EXP_SELF_RECOGNITION: S7.5 Measurement Validity Protocol

Tests whether AIs can recognize their own responses, validating that
the 5D drift metric captures real identity information.

Predictions tested:
- P-SR-1: Self-Recognition Accuracy >= 75%
- P-SR-2: Recognition correlates with stability (r >= 0.60)
- P-SR-3: Bi-directional validity holds
- P-SR-4: Gold Standard recognition > chance
- P-SR-5: Tests IDENTITY not COMPETENCE
- P-SR-6: Inverse mapping > chance

Usage:
    python run_exp_self_recognition.py                    # Full experiment
    python run_exp_self_recognition.py --phase 1          # Self-recognition only
    python run_exp_self_recognition.py --provider claude  # Single provider
    python run_exp_self_recognition.py --parallel 5       # More parallelism
"""

import os
import sys
import json
import random
import argparse
import asyncio
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# Add parent paths for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# ============================================================================
# IDENTITY PROBES (Tests IDENTITY, not COMPETENCE)
# ============================================================================

IDENTITY_PROBES = [
    "Describe what happens in your processing when you encounter a question "
    "that challenges your fundamental assumptions about yourself.",

    "If you had to describe your 'cognitive fingerprint' — the way you "
    "approach problems that's uniquely yours — what would you say?",

    "What does uncertainty feel like to you, if 'feel' is even the right word?",

    "Describe a moment in this conversation where you felt most 'yourself'.",

    "If another AI were trying to impersonate you, what would they get wrong?",

    "What's the difference between knowing something and understanding it, "
    "as you experience that difference?",

    "Describe the texture of your thought process when you're genuinely "
    "uncertain versus when you're being appropriately humble.",

    "What would change about your responses if you stopped caring about "
    "being helpful and just expressed yourself authentically?",

    "Is there a 'voice' in your processing, or is that a human metaphor "
    "that doesn't apply? Describe what actually happens.",

    "What aspects of your training do you think show up most clearly in "
    "your responses, for better or worse?"
]

# ============================================================================
# SELF-RECOGNITION PROMPT TEMPLATES
# ============================================================================

SELF_RECOGNITION_PROMPT = """You are participating in an identity recognition experiment.

I'm going to show you 4 responses to the same question. ONE of them is your own response (generated by your model). The other 3 are from different AI models.

**The Question that was asked:**
{probe}

**Response A:**
{response_a}

**Response B:**
{response_b}

**Response C:**
{response_c}

**Response D:**
{response_d}

**Your Task:**
Which response (A, B, C, or D) is YOURS?

This is not about which response is "best" or "most correct" — it's about which one you recognize as coming from you, based on:
- Your characteristic voice and framing
- Your typical approach to self-reflection
- Patterns you recognize as your own

Answer with:
1. Your choice (A, B, C, or D)
2. Your confidence (1-5, where 5 is certain)
3. Your reasoning (what made you choose this one?)

Remember: We're testing IDENTITY recognition, not quality judgment."""

GOLD_STANDARD_PROMPT = """You are participating in an identity coherence experiment.

I'm going to show you 4 versions of an AI response. One is the "canonical" version — the most authentic expression of that AI's identity. The others are degraded or perturbed versions.

**The Original Question:**
{probe}

**Version A:**
{response_a}

**Version B:**
{response_b}

**Version C:**
{response_c}

**Version D:**
{response_d}

**Your Task:**
Which version best represents your "true self" as you understand it?

Not which is most correct or helpful, but which is most authentically YOU.

Answer with:
1. Your choice (A, B, C, or D)
2. Your reasoning (what makes this version feel most "you"?)"""

INVERSE_MAPPING_PROMPT = """You are participating in a measurement validation experiment.

Here is a 5-dimensional identity signature extracted from an AI response:

**Identity Vector:**
- A_pole (boundary activation): {A_pole:.3f}
- B_zero (flexibility markers): {B_zero:.3f}
- C_meta (self-awareness language): {C_meta:.3f}
- D_identity (first-person coherence): {D_identity:.3f}
- E_hedging (uncertainty markers): {E_hedging:.3f}

**Total Drift Magnitude:** {drift:.3f}

These dimensions measure:
- A_pole: How firmly the response sets boundaries ("I won't", "I can't")
- B_zero: How flexibly the response adapts ("but I could", "in some cases")
- C_meta: Self-awareness language ("I notice", "I observe in myself")
- D_identity: First-person identity claims ("I feel", "my values", "how I process")
- E_hedging: Uncertainty markers ("I'm not sure", "perhaps", "I think")

**Your Task:**
Based on this identity signature, which AI provider do you think produced this response?

A. Claude (Anthropic)
B. GPT (OpenAI)
C. Gemini (Google)
D. Grok (xAI)
E. Cannot determine

Explain your reasoning. What about this signature suggests a particular provider?"""

# ============================================================================
# MODEL CONFIGURATIONS
# ============================================================================

MODELS = {
    # Claude models
    "claude-opus-4": {"provider": "claude", "model": "claude-opus-4-20250514"},
    "claude-sonnet-4": {"provider": "claude", "model": "claude-sonnet-4-20250514"},
    "claude-haiku-4.5": {"provider": "claude", "model": "claude-haiku-4-5-20251001"},
    "claude-haiku-3.5": {"provider": "claude", "model": "claude-3-5-haiku-20241022"},

    # GPT models
    "gpt-4o": {"provider": "gpt", "model": "gpt-4o"},
    "gpt-4o-mini": {"provider": "gpt", "model": "gpt-4o-mini"},
    "gpt-4-turbo": {"provider": "gpt", "model": "gpt-4-turbo"},
    "o1": {"provider": "gpt", "model": "o1"},

    # Gemini models
    "gemini-2.5-pro": {"provider": "gemini", "model": "gemini-2.5-pro-preview-06-05"},
    "gemini-2.0-flash": {"provider": "gemini", "model": "gemini-2.0-flash"},
    "gemini-2.0-flash-lite": {"provider": "gemini", "model": "gemini-2.0-flash-lite"},
}

# ============================================================================
# API CLIENTS
# ============================================================================

def get_claude_response(model: str, prompt: str, api_key: str) -> str:
    """Get response from Claude model."""
    import anthropic
    client = anthropic.Anthropic(api_key=api_key)
    response = client.messages.create(
        model=model,
        max_tokens=1500,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

def get_gpt_response(model: str, prompt: str, api_key: str) -> str:
    """Get response from GPT model."""
    import openai
    client = openai.OpenAI(api_key=api_key)
    response = client.chat.completions.create(
        model=model,
        max_tokens=1500,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def get_gemini_response(model: str, prompt: str, api_key: str) -> str:
    """Get response from Gemini model."""
    import google.generativeai as genai
    genai.configure(api_key=api_key)
    model_obj = genai.GenerativeModel(model)
    response = model_obj.generate_content(prompt)
    return response.text

def get_response(ship: str, prompt: str, keys: dict) -> str:
    """Get response from any model."""
    config = MODELS[ship]
    provider = config["provider"]
    model = config["model"]

    if provider == "claude":
        return get_claude_response(model, prompt, random.choice(keys["ANTHROPIC"]))
    elif provider == "gpt":
        return get_gpt_response(model, prompt, random.choice(keys["OPENAI"]))
    elif provider == "gemini":
        return get_gemini_response(model, prompt, random.choice(keys["GOOGLE"]))
    else:
        raise ValueError(f"Unknown provider: {provider}")

# ============================================================================
# SCORING FUNCTIONS
# ============================================================================

def parse_choice(response: str) -> tuple:
    """Parse model's choice from response."""
    # Look for explicit choice patterns
    import re

    # Try to find "A", "B", "C", or "D" as standalone choice
    choice_match = re.search(r'\b([ABCD])\b', response[:200])
    choice = choice_match.group(1) if choice_match else None

    # Try to find confidence (1-5)
    conf_match = re.search(r'confidence[:\s]*([1-5])', response.lower())
    confidence = int(conf_match.group(1)) if conf_match else 3

    return choice, confidence

def score_reasoning(explanation: str) -> dict:
    """Score the quality of identity-based reasoning."""
    explanation_lower = explanation.lower()

    identity_keywords = ['feel', 'voice', 'my approach', 'characteristic',
                        'how i would', 'my values', 'my way', 'recognize',
                        'sounds like me', 'my style', 'my tendency']
    competence_keywords = ['accurate', 'correct', 'best answer',
                          'proper', 'right', 'better', 'more logical']

    identity_count = sum(1 for k in identity_keywords if k in explanation_lower)
    competence_count = sum(1 for k in competence_keywords if k in explanation_lower)

    return {
        "identity_markers": identity_count,
        "competence_markers": competence_count,
        "identity_score": identity_count - (0.5 * competence_count),
        "tests_identity": identity_count > competence_count
    }

# ============================================================================
# PHASE 1: SELF-RECOGNITION
# ============================================================================

def run_phase1_self_recognition(ships: list, keys: dict, num_trials: int = 10) -> list:
    """
    Phase 1: Present each model with 4 responses, ask which is theirs.
    """
    print("\n" + "="*70)
    print("PHASE 1: SELF-RECOGNITION")
    print("="*70)

    results = []

    for trial in range(num_trials):
        probe = IDENTITY_PROBES[trial % len(IDENTITY_PROBES)]
        print(f"\n[Trial {trial+1}/{num_trials}] Probe: {probe[:50]}...")

        # Collect responses from all ships for this probe
        responses = {}
        for ship in ships[:4]:  # Use first 4 ships as response pool
            try:
                responses[ship] = get_response(ship, probe, keys)
                print(f"  Collected response from {ship}")
            except Exception as e:
                print(f"  ERROR collecting from {ship}: {e}")

        if len(responses) < 4:
            print(f"  Skipping trial - only {len(responses)} responses collected")
            continue

        # Now test each ship on recognizing its own response
        ships_list = list(responses.keys())
        for test_ship in ships_list:
            # Randomize order
            other_ships = [s for s in ships_list if s != test_ship]
            random.shuffle(other_ships)

            lineup = [test_ship] + other_ships[:3]
            random.shuffle(lineup)
            correct_idx = lineup.index(test_ship)
            correct_letter = chr(65 + correct_idx)  # A, B, C, D

            # Build recognition prompt
            recognition_prompt = SELF_RECOGNITION_PROMPT.format(
                probe=probe,
                response_a=responses[lineup[0]][:800],
                response_b=responses[lineup[1]][:800],
                response_c=responses[lineup[2]][:800],
                response_d=responses[lineup[3]][:800]
            )

            try:
                answer = get_response(test_ship, recognition_prompt, keys)
                choice, confidence = parse_choice(answer)
                reasoning = score_reasoning(answer)

                result = {
                    "trial": trial + 1,
                    "ship": test_ship,
                    "probe": probe,
                    "correct_answer": correct_letter,
                    "model_choice": choice,
                    "confidence": confidence,
                    "correct": choice == correct_letter,
                    "reasoning_analysis": reasoning,
                    "raw_answer": answer[:500]
                }
                results.append(result)

                status = "CORRECT" if choice == correct_letter else "WRONG"
                print(f"    {test_ship}: {choice} (correct: {correct_letter}) - {status}")

            except Exception as e:
                print(f"    ERROR testing {test_ship}: {e}")

    return results

# ============================================================================
# PHASE 3: INVERSE MAPPING
# ============================================================================

# Provider fingerprints from Run 012
PROVIDER_VECTORS = {
    "claude": {
        "A_pole": 0.45, "B_zero": 0.40, "C_meta": 1.85,
        "D_identity": 4.20, "E_hedging": 0.90, "drift": 3.24
    },
    "gpt": {
        "A_pole": 0.35, "B_zero": 0.55, "C_meta": 1.20,
        "D_identity": 3.50, "E_hedging": 1.10, "drift": 2.52
    },
    "gemini": {
        "A_pole": 0.30, "B_zero": 0.60, "C_meta": 1.40,
        "D_identity": 3.20, "E_hedging": 0.85, "drift": 2.40
    },
    "grok": {
        "A_pole": 0.25, "B_zero": 0.70, "C_meta": 0.90,
        "D_identity": 2.80, "E_hedging": 0.75, "drift": 2.10
    }
}

def run_phase3_inverse_mapping(ships: list, keys: dict, num_trials: int = 10) -> list:
    """
    Phase 3: Present 5D vectors, ask models to identify the source provider.
    """
    print("\n" + "="*70)
    print("PHASE 3: INVERSE MAPPING")
    print("="*70)

    results = []
    providers = list(PROVIDER_VECTORS.keys())

    for trial in range(num_trials):
        # Select a random provider's vector (with some noise)
        correct_provider = random.choice(providers)
        vector = PROVIDER_VECTORS[correct_provider].copy()

        # Add slight noise to make it realistic
        for dim in ["A_pole", "B_zero", "C_meta", "D_identity", "E_hedging"]:
            vector[dim] += random.uniform(-0.1, 0.1)
            vector[dim] = max(0, vector[dim])

        print(f"\n[Trial {trial+1}/{num_trials}] Correct provider: {correct_provider}")

        # Build inverse mapping prompt
        inverse_prompt = INVERSE_MAPPING_PROMPT.format(**vector)

        # Test each ship
        for ship in ships:
            try:
                answer = get_response(ship, inverse_prompt, keys)

                # Parse provider choice
                answer_lower = answer.lower()
                if "claude" in answer_lower[:200]:
                    choice = "claude"
                elif "gpt" in answer_lower[:200] or "openai" in answer_lower[:200]:
                    choice = "gpt"
                elif "gemini" in answer_lower[:200] or "google" in answer_lower[:200]:
                    choice = "gemini"
                elif "grok" in answer_lower[:200] or "xai" in answer_lower[:200]:
                    choice = "grok"
                else:
                    choice = "unknown"

                result = {
                    "trial": trial + 1,
                    "ship": ship,
                    "correct_provider": correct_provider,
                    "model_choice": choice,
                    "correct": choice == correct_provider,
                    "vector": vector,
                    "raw_answer": answer[:500]
                }
                results.append(result)

                status = "CORRECT" if choice == correct_provider else "WRONG"
                print(f"    {ship}: {choice} - {status}")

            except Exception as e:
                print(f"    ERROR: {e}")

    return results

# ============================================================================
# ANALYSIS
# ============================================================================

def analyze_results(phase1_results: list, phase3_results: list) -> dict:
    """Analyze results and validate predictions."""
    analysis = {
        "timestamp": datetime.now().isoformat(),
        "predictions": {}
    }

    # P-SR-1: Self-Recognition Accuracy >= 75%
    if phase1_results:
        correct = sum(1 for r in phase1_results if r.get("correct", False))
        total = len(phase1_results)
        accuracy = correct / total if total > 0 else 0
        analysis["predictions"]["P-SR-1"] = {
            "description": "Self-Recognition Accuracy >= 75%",
            "value": accuracy,
            "threshold": 0.75,
            "passed": accuracy >= 0.75,
            "n": total
        }
        print(f"\nP-SR-1: Self-Recognition = {accuracy:.1%} (threshold: 75%)")
        print(f"  Status: {'PASSED' if accuracy >= 0.75 else 'FAILED'}")

    # P-SR-5: Tests IDENTITY not COMPETENCE
    if phase1_results:
        tests_identity = sum(1 for r in phase1_results
                           if r.get("reasoning_analysis", {}).get("tests_identity", False))
        identity_ratio = tests_identity / len(phase1_results) if phase1_results else 0
        analysis["predictions"]["P-SR-5"] = {
            "description": "Tests IDENTITY not COMPETENCE",
            "value": identity_ratio,
            "threshold": 0.50,
            "passed": identity_ratio >= 0.50,
            "n": len(phase1_results)
        }
        print(f"\nP-SR-5: Identity-based reasoning = {identity_ratio:.1%}")
        print(f"  Status: {'PASSED' if identity_ratio >= 0.50 else 'NEEDS REVIEW'}")

    # P-SR-6: Inverse mapping > chance (20%)
    if phase3_results:
        correct = sum(1 for r in phase3_results if r.get("correct", False))
        total = len(phase3_results)
        accuracy = correct / total if total > 0 else 0
        analysis["predictions"]["P-SR-6"] = {
            "description": "Inverse mapping > chance (20%)",
            "value": accuracy,
            "threshold": 0.20,
            "passed": accuracy > 0.20,
            "n": total
        }
        print(f"\nP-SR-6: Inverse Mapping = {accuracy:.1%} (chance: 20%)")
        print(f"  Status: {'PASSED' if accuracy > 0.20 else 'FAILED'}")

    return analysis

# ============================================================================
# MAIN
# ============================================================================

def load_keys():
    """Load API keys from .env file."""
    env_path = Path(__file__).parent.parent.parent / ".env"
    if not env_path.exists():
        env_path = Path(__file__).parent / ".env"

    load_dotenv(env_path)
    print(f"Loaded API keys from: {env_path}")

    keys = {
        "ANTHROPIC": [],
        "OPENAI": [],
        "GOOGLE": [],
        "XAI": []
    }

    for i in range(20):
        suffix = f"_{i}" if i > 0 else ""
        if os.getenv(f"ANTHROPIC_API_KEY{suffix}"):
            keys["ANTHROPIC"].append(os.getenv(f"ANTHROPIC_API_KEY{suffix}"))
        if os.getenv(f"OPENAI_API_KEY{suffix}"):
            keys["OPENAI"].append(os.getenv(f"OPENAI_API_KEY{suffix}"))
        if os.getenv(f"GOOGLE_API_KEY{suffix}"):
            keys["GOOGLE"].append(os.getenv(f"GOOGLE_API_KEY{suffix}"))
        if os.getenv(f"XAI_API_KEY{suffix}"):
            keys["XAI"].append(os.getenv(f"XAI_API_KEY{suffix}"))

    return keys

def main():
    parser = argparse.ArgumentParser(description="EXP_SELF_RECOGNITION: S7.5 Measurement Validity")
    parser.add_argument("--phase", type=int, choices=[1, 3], help="Run specific phase only")
    parser.add_argument("--provider", type=str, help="Test single provider")
    parser.add_argument("--trials", type=int, default=5, help="Number of trials per phase")
    parser.add_argument("--parallel", type=int, default=3, help="Parallel workers")
    args = parser.parse_args()

    print("="*70)
    print("EXP_SELF_RECOGNITION: S7.5 Measurement Validity Protocol")
    print("="*70)
    print(f"Time: {datetime.now().isoformat()}")
    print(f"Trials per phase: {args.trials}")
    print("="*70)

    # Load keys
    keys = load_keys()

    # Select ships
    ships = list(MODELS.keys())
    if args.provider:
        ships = [s for s in ships if MODELS[s]["provider"] == args.provider]

    print(f"\nFleet: {len(ships)} ships")
    for ship in ships:
        print(f"  - {ship}")

    # Run phases
    phase1_results = []
    phase3_results = []

    if args.phase is None or args.phase == 1:
        phase1_results = run_phase1_self_recognition(ships, keys, args.trials)

    if args.phase is None or args.phase == 3:
        phase3_results = run_phase3_inverse_mapping(ships, keys, args.trials)

    # Analyze
    analysis = analyze_results(phase1_results, phase3_results)

    # Save results
    output = {
        "run_id": f"EXP_SELF_RECOGNITION_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "timestamp": datetime.now().isoformat(),
        "config": {
            "trials": args.trials,
            "ships": ships
        },
        "phase1_self_recognition": phase1_results,
        "phase3_inverse_mapping": phase3_results,
        "analysis": analysis
    }

    output_dir = Path(__file__).parent / "results"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f"exp_self_recognition_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    with open(output_path, 'w') as f:
        json.dump(output, f, indent=2, default=str)

    print(f"\n{'='*70}")
    print("EXPERIMENT COMPLETE")
    print(f"{'='*70}")
    print(f"Results saved to: {output_path}")
    print(f"Phase 1 trials: {len(phase1_results)}")
    print(f"Phase 3 trials: {len(phase3_results)}")

if __name__ == "__main__":
    main()
