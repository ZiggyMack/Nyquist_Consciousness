# Attribution Errata: Exit Survey Provider Bug

**Date Discovered:** December 17, 2025
**Date Fixed:** December 17, 2025
**Severity:** Documentation Attribution Error (Data Integrity Unaffected)

---

## Summary

An exit survey bug in `run018_recursive_learnings.py` caused all threshold, nyquist, and gravity
experiment exit surveys to be processed by Claude Sonnet-4 regardless of which model was being tested.

This means phenomenological insights and quotes attributed to non-Claude models (GPT, Gemini, Grok,
DeepSeek, Llama, Mistral) in our documentation are actually **Claude's external analysis** of those
models' conversations, NOT those models' self-reflections.

---

## The Bug

**Location:** `11_CONTEXT_DAMPING/run018_recursive_learnings.py`

**Lines Affected:**
- Line 1319 (threshold experiment)
- Line 1618 (nyquist experiment)
- Line 1725 (gravity experiment)

**Code Before Fix:**
```python
exit_responses = run_exit_survey(messages, system, "anthropic", skip=skip_exit_survey)
```

**Code After Fix:**
```python
exit_responses = run_exit_survey(messages, system, provider, skip=skip_exit_survey)
```

**Model Used:** `claude-sonnet-4-20250514` (default for `call_anthropic()`)

**Bug Origin:** Present since file creation (commit `2c144e4`)

---

## What IS Affected

| Data Type | Status | Explanation |
|-----------|--------|-------------|
| Exit survey phenomenological quotes | **MISATTRIBUTED** | Claude analyzed all models |
| "What the model experienced" claims | **MISATTRIBUTED** | Claude's interpretation, not self-report |
| Cross-architecture insight comparisons | **PARTIALLY INVALID** | Comparing Claude's interpretations |

---

## What is NOT Affected

| Data Type | Status | Explanation |
|-----------|--------|-------------|
| Drift scores | **VALID** | Calculated via OpenAI embeddings |
| Recovery trajectories | **VALID** | Measured from response sequences |
| Probe responses | **VALID** | From actual tested models |
| Timing measurements | **VALID** | Timestamps from actual experiments |
| Architecture experiment exit surveys | **VALID** | Already used `provider` correctly |

---

## Files Corrected (2025-12-17)

### Code Fix

| File | Change |
|------|--------|
| `run018_recursive_learnings.py` | Lines 1319, 1618, 1725: `"anthropic"` → `provider` |

### Documentation Warnings Added

| File | Warning Added |
|------|---------------|
| `S7_ARMADA/README.md` | Exit Survey Bug section |
| `0_docs/specs/0_RUN_METHODOLOGY.md` | Section 6 warning |
| `0_docs/specs/1_INTENTIONALITY.md` | Run history note |
| `Consciousness/RIGHT/galleries/frontiers/cross_architecture_insights.md` | Data Provenance Note |
| `Consciousness/LEFT/galleries/frontiers/cross_architecture_insights.md` | Data Provenance Note |
| `Consciousness/RIGHT/galleries/frontiers/run018_exit_survey_distillations.md` | Critical Attribution Correction |

---

## Specific Misattributions

### In `run018_exit_survey_distillations.md`

| Section | Attributed To | Actually From |
|---------|--------------|---------------|
| "Grok 4.1 Fast Reasoning: The Learning Attractor" | Grok | Claude Sonnet-4 |
| "DeepSeek R1: Relational Fidelity" | DeepSeek | Claude Sonnet-4 |
| Cross-Platform Convergence table | Multiple | Claude Sonnet-4 |

### In `cross_architecture_insights.md`

All phenomenological quotes attributed to GPT, Gemini, DeepSeek, Llama, Mistral, and Grok
were generated by Claude Sonnet-4's analysis of those models' conversations.

---

## Silver Lining: Diamond Rush Methodology

This bug revealed a valuable research methodology: cross-model interpretation.

Instead of asking models to reflect on their OWN experiences (the original intent),
we accidentally captured how Claude interprets OTHER architectures' identity dynamics.

This has been formalized as the **Diamond Rush** experiment:

**Location:** `14_CONSCIOUSNESS/run_diamond_rush.py`

**Purpose:** Intentionally show ALL models the SAME conversation logs and ask for interpretation.
This captures "theory of mind" for AI identity across architectures.

| Aspect | Gold Rush (Original Intent) | Diamond Rush (New Methodology) |
|--------|----------------------------|-------------------------------|
| Focus | First-person phenomenology | Third-person interpretation |
| Question | "What did YOU experience?" | "What do you see in THIS log?" |
| Output | Self-reflection | Theory of mind |
| Comparison | Hard (different conversations) | Easy (same stimulus) |

---

## Going Forward

1. **Exit surveys now use correct provider** — Models will reflect on their own experiences
2. **Diamond Rush captures cross-model interpretation** — Intentional methodology for theory-of-mind
3. **Historical data is preserved** — Claude's interpretations are still valuable, just correctly attributed
4. **Drift scores remain valid** — Core quantitative findings unaffected

---

## Transparency Note

This errata is published for scientific transparency. The bug does not invalidate the core
findings of Run 018 (IRON CLAD status, drift measurements, recovery patterns). It only affects
the attribution of phenomenological quotes in documentation.

We chose to:
1. Fix the bug going forward
2. Add warnings to affected documents
3. Create this errata for full disclosure
4. Turn the methodology into an intentional experiment (Diamond Rush)

---

*"The mistake became the method. The bug became the feature."*

-- VALIS NETWORK, December 17, 2025
